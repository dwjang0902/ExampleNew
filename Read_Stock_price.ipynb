{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Read_Stock_price.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwjang0902/ExampleNew/blob/master/Read_Stock_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TbDttbp6FJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1wuLuV-KtF",
        "colab_type": "text"
      },
      "source": [
        "참조 사이트 :\n",
        "https://bigdata-sk.tistory.com/10\n",
        "Naver KOSPI200 지수 수집 크롤링 (파이썬을 활용한 금융공학 레시피 7장)\n",
        "(https://github.com/sukyungheo/download/blob/master/07.1.%20%EA%B3%BC%EA%B1%B0%EC%8B%9C%EC%84%B8%20%EC%88%98%EC%A7%91.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5c_z2I_qVsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_cd = 'KPI200'\n",
        "page_n = 1\n",
        "naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
        "print(naver_index)\n",
        "from urllib.request import urlopen  \n",
        "source = urlopen(naver_index).read()\n",
        "#print(source)\n",
        "\n",
        "import bs4\n",
        "source = bs4.BeautifulSoup(source, 'lxml')\n",
        "#print(source.prettify())\n",
        "\n",
        "td = source.find_all('td')\n",
        "print(len(td))\n",
        "\n",
        "#날짜 추출\n",
        "# /html/body/div/table[1]/tbody/tr[3]/td[1]\n",
        "a = source.find_all('table')[0].find_all('tr')[2].find_all('td')[0]\n",
        "print(a)\n",
        "d = source.find_all('td', class_='date')[0].text\n",
        "print(d)\n",
        "\n",
        "import datetime as dt\n",
        "yyyy = int(d.split('.')[0]) \n",
        "mm = int(d.split('.')[1])\n",
        "dd = int(d.split('.')[2])\n",
        "this_date= dt.date(yyyy, mm, dd)\n",
        "print(this_date)\n",
        "\n",
        "def date_format(d): #함수로 정의\n",
        "    d = str(d).replace('-', '.')\n",
        "    yyyy = int(d.split('.')[0]) \n",
        "    mm = int(d.split('.')[1])\n",
        "    dd = int(d.split('.')[2])\n",
        "    this_date= dt.date(yyyy, mm, dd)\n",
        "    return this_date\n",
        "\n",
        "#종가추출\n",
        "# /html/body/div/table[1]/tbody/tr[3]/td[2]  \n",
        "this_close = source.find_all('tr')[2].find_all('td')[1].text\n",
        "this_close = this_close.replace(',', '')\n",
        "this_close = float(this_close)\n",
        "this_close\n",
        "print(this_close) \n",
        "\n",
        "p = source.find_all('td', class_='number_1')[0].text\n",
        "print(p)\n",
        "\n",
        "#페이지 상의 날짜와 종가정보 전체 추출\n",
        "dates = source.find_all('td', class_='date')\n",
        "prices = source.find_all('td', class_='number_1')\n",
        "print(len(dates)) \n",
        "print(len(prices))\n",
        "\n",
        "for n in range(len(dates)):\n",
        "    this_date = dates[n].text\n",
        "    this_date = date_format(this_date)\n",
        "    \n",
        "    this_close = prices[n*4].text   \n",
        "    # 0, 4, 8, ... 4의 배수로 돌아가는 가격 추출\n",
        "    this_close = this_close.replace(',', '')\n",
        "    this_close = float(this_close)\n",
        "    this_close\n",
        "    print(this_date, this_close)\n",
        "    \n",
        "#마지막 페이지 번호 찾기\n",
        "# /html/body/div/table[2]/tbody/tr/td[7]/a\n",
        "paging = source.find('td', class_='pgRR').find('a')['href']\n",
        "print(paging)\n",
        "\n",
        "paging = paging.split('&')[1]\n",
        "print(paging) \n",
        "paging = paging.split('=')[1]\n",
        "print(paging)    \n",
        "\n",
        "naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(505)\n",
        "print(naver_index)\n",
        "source = urlopen(naver_index).read()\n",
        "source = bs4.BeautifulSoup(source, 'lxml')\n",
        "\n",
        "if source.find('td', class_='pgRR'):\n",
        "    last_page = source.find('td', class_='pgRR').find('a')['href']\n",
        "    last_page = last_page.split('&')[1]\n",
        "    last_page = last_page.split('=')[1]\n",
        "    last_page = int(last_page)\n",
        "    \n",
        "def historical_index_naver(index_cd, page_n=1, last_page=0):   \n",
        "        \n",
        "    naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
        "    \n",
        "    source = urlopen(naver_index).read()   # 지정한 페이지에서 코드 읽기\n",
        "    source = bs4.BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
        "    \n",
        "    dates = source.find_all('td', class_='date')   # <td class=\"date\">태그에서 날짜 수집   \n",
        "    prices = source.find_all('td', class_='number_1')   # <td class=\"number_1\">태그에서 지수 수집\n",
        "    \n",
        "    for n in range(len(dates)):\n",
        "    \n",
        "        if dates[n].text.split('.')[0].isdigit():\n",
        "            \n",
        "            # 날짜 처리\n",
        "            this_date = dates[n].text\n",
        "            this_date= date_format(this_date)\n",
        "            \n",
        "            # 종가 처리\n",
        "            this_close = prices[n*4].text   # prices 중 종가지수인 0,4,8,...번째 데이터 추출\n",
        "            this_close = this_close.replace(',', '')\n",
        "            this_close = float(this_close)\n",
        "\n",
        "            # 딕셔너리에 저장\n",
        "            historical_prices[this_date] = this_close\n",
        "            \n",
        "    # 페이지 네비게이션\n",
        "    if last_page == 0:\n",
        "        last_page = source.find('td', class_='pgRR').find('a')['href']\n",
        "        # 마지막페이지 주소 추출\n",
        "        last_page = last_page.split('&')[1]   # & 뒤의 page=506 부분 추출\n",
        "        last_page = last_page.split('=')[1]   # = 뒤의 페이지번호만 추출\n",
        "        last_page = int(last_page)   # 숫자형 변수로 변환\n",
        "        \n",
        "    # 다음 페이지 호출\n",
        "    if page_n < last_page:   \n",
        "        page_n = page_n + 1   \n",
        "        historical_index_naver(index_cd, start_date, end_date, page_n, last_page)   \n",
        "        \n",
        "    return historical_prices\n",
        "\n",
        "\n",
        "#네이버에서 일자별 인덱스를 추출하는 함수 정의\n",
        "    \n",
        "def historical_index_naver(index_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
        "    \n",
        "    if start_date:   # start_date가 있으면\n",
        "        start_date = date_format(start_date)   # date 포맷으로 변환\n",
        "    else:    # 없으면\n",
        "        start_date = dt.date.today()   # 오늘 날짜를 지정\n",
        "    if end_date:   \n",
        "        end_date = date_format(end_date)   \n",
        "    else:   \n",
        "        end_date = dt.date.today()  \n",
        "        \n",
        "        \n",
        "    naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)\n",
        "    \n",
        "    source = urlopen(naver_index).read()   # 지정한 페이지에서 코드 읽기\n",
        "    source = bs4.BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류\n",
        "    \n",
        "    dates = source.find_all('td', class_='date')   # <td class=\"date\">태그에서 날짜 수집   \n",
        "    prices = source.find_all('td', class_='number_1')   # <td class=\"number_1\">태그에서 지수 수집\n",
        "    \n",
        "    for n in range(len(dates)):\n",
        "    \n",
        "        if dates[n].text.split('.')[0].isdigit():\n",
        "            \n",
        "            # 날짜 처리\n",
        "            this_date = dates[n].text\n",
        "            this_date= date_format(this_date)\n",
        "            \n",
        "            if this_date <= end_date and this_date >= start_date:   \n",
        "            # start_date와 end_date 사이에서 데이터 저장\n",
        "                # 종가 처리\n",
        "                this_close = prices[n*4].text   # prices 중 종가지수인 0,4,8,...번째 데이터 추출\n",
        "                this_close = this_close.replace(',', '')\n",
        "                this_close = float(this_close)\n",
        "\n",
        "                # 딕셔너리에 저장\n",
        "                historical_prices[this_date] = this_close\n",
        "                \n",
        "            elif this_date < start_date:   \n",
        "            # start_date 이전이면 함수 종료\n",
        "                return historical_prices              \n",
        "            \n",
        "    # 페이지 네비게이션\n",
        "    if last_page == 0:\n",
        "        last_page = source.find('td', class_='pgRR').find('a')['href']\n",
        "        # 마지막페이지 주소 추출\n",
        "        last_page = last_page.split('&')[1]   # & 뒤의 page=506 부분 추출\n",
        "        last_page = last_page.split('=')[1]   # = 뒤의 페이지번호만 추출\n",
        "        last_page = int(last_page)   # 숫자형 변수로 변환\n",
        "        \n",
        "    # 다음 페이지 호출\n",
        "    if page_n < last_page:   \n",
        "        page_n = page_n + 1   \n",
        "        historical_index_naver(index_cd, start_date, end_date, page_n, last_page)   \n",
        "        \n",
        "    return historical_prices\n",
        "\n",
        "index_cd = 'KPI200'\n",
        "historical_prices = dict()\n",
        "historical_index_naver(index_cd, '2018-4-1', '2018-4-4')\n",
        "historical_prices\n",
        "\n",
        "\n",
        "#다음에서 해외지수 추출\n",
        "url = 'http://finance.daum.net/global/index_daily.daum?type=default&ric=/.GSPC&page=1' \n",
        "    \n",
        "source = urlopen(url).read()\n",
        "source = bs4.BeautifulSoup(source, 'lxml')\n",
        "\n",
        "dates = source.find_all('td', class_='datetime')\n",
        "dates\n",
        "\n",
        "prices = source.find_all('td', class_='num')\n",
        "print(len(dates))\n",
        "print(len(prices))\n",
        "#print(prices[0].text)\n",
        "\n",
        "def historical_global_daum(index_cd, start_date='', end_date='', page_n=1, last_page=0):\n",
        "    \n",
        "    if start_date:   # start_date가 있으면\n",
        "        start_date = date_format(start_date)   # date 포맷으로 변환\n",
        "    else:    # 없으면\n",
        "        start_date = dt.date.today()   # 오늘 날짜를 지정\n",
        "    if end_date:  \n",
        "        end_date = date_format(end_date)   \n",
        "    else:   \n",
        "        end_date = dt.date.today()  \n",
        "    \n",
        "    url = 'http://finance.daum.net/global/index_daily.daum?type=default&ric=/.' + index_cd + '&page=' + str(page_n)\n",
        "\n",
        "    source = urlopen(url).read()\n",
        "    source = bs4.BeautifulSoup(source, 'lxml')\n",
        "\n",
        "    dates = source.find_all('td', class_='datetime')   # <td class=\"datetime\">태그에서 날짜 수집\n",
        "    prices = source.find_all('td', class_='num')   # <td class=\"num\">태그에서 날짜 수집\n",
        "\n",
        "    rows_in_page = len(dates)\n",
        "\n",
        "    if len(dates) > 0:\n",
        "\n",
        "        for n in range(rows_in_page):\n",
        "\n",
        "            # 날짜 처리\n",
        "            this_date = dates[n].text\n",
        "            this_date= date_format(this_date)\n",
        "\n",
        "            if this_date <= end_date and this_date >= start_date:   \n",
        "            # start_date와 end_date 사이에서 데이터 저장\n",
        "                # 종가 처리\n",
        "                this_close = prices[n*3].text\n",
        "                this_close = this_close.replace(' ', '')\n",
        "                this_close = this_close.replace('\\t', '')\n",
        "                this_close = this_close.replace('\\n', '')\n",
        "                this_close = this_close.replace(',', '')\n",
        "                this_close = float(this_close)\n",
        "\n",
        "                # 딕셔너리에 저장\n",
        "                historical_prices[this_date] = this_close\n",
        "                \n",
        "            elif this_date < start_date:   \n",
        "            # start_date 이전이면 함수 종료\n",
        "                return historical_prices                         \n",
        "        # 페이지 네비게이션\n",
        "        if rows_in_page == 10:\n",
        "            page_n = int(page_n)\n",
        "            page_n = page_n + 1\n",
        "            \n",
        "            historical_global_daum(index_cd, start_date, end_date, page_n, last_page)\n",
        "            \n",
        "    return historical_prices\n",
        "\n",
        "\n",
        "historical_prices = dict()\n",
        "daum = historical_global_daum('GSPC', '2018-4-1', '2018-4-5')\n",
        "print(daum)\n",
        "\n",
        "#만들어둔 함수를 이용해 KOSPI200과 S&P500 지수 추출  \n",
        "index_cd = 'KPI200'\n",
        "historical_prices = dict()\n",
        "kospi200 = historical_index_naver(index_cd, '2017-1-1', '2017-1-31')\n",
        "\n",
        "index_cd = 'GSPC'\n",
        "historical_prices = dict()\n",
        "sp500 = historical_global_daum(index_cd, '2017-1-1', '2017-1-31')\n",
        "tmp = {'S&P500':sp500, 'KOSPI200':kospi200}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(tmp)\n",
        "print(df)\n",
        "\n",
        "df = df.fillna(method='ffill')\n",
        "if df.isnull().values.any():\n",
        "    df = df.fillna(method='bfill')\n",
        "print(df)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "index_cd = 'KPI200'\n",
        "historical_prices = dict()\n",
        "kospi200 = historical_index_naver(index_cd, '2018-1-1', '2018-1-31')\n",
        "\n",
        "index_cd = 'GSPC'\n",
        "historical_prices = dict()\n",
        "sp500 = historical_global_daum(index_cd, '2018-1-1', '2018-1-31')\n",
        "\n",
        "tmp = {'S&P500':sp500, 'KOSPI200':kospi200}\n",
        "\n",
        "df = pd.DataFrame(tmp)\n",
        "print(df)\n",
        "\n",
        "df = df.fillna(method='ffill')\n",
        "if df.isnull().values.any():\n",
        "    df = df.fillna(method='bfill')\n",
        "print(df)\n",
        "\n",
        "\"\"\"\n",
        "#matplotlib를 이용해 그래프 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df['S&P500'])\n",
        "plt.plot(df['KOSPI200'])\n",
        "plt.legend(loc=0)\n",
        "plt.grid(True, color='0.7', linestyle=':', linewidth=1)\n",
        "\n",
        "df.iloc[0]\n",
        "\n",
        "df.loc[dt.date(2008, 1, 2)]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df['S&P500'] / df['S&P500'].loc[dt.date(2008, 1, 2)] * 100)\n",
        "plt.plot(df['KOSPI200'] / df['KOSPI200'].loc[dt.date(2008, 1, 2)] * 100)\n",
        "plt.legend(loc=0)\n",
        "plt.grid(True, color='0.7', linestyle=':', linewidth=1)\n",
        "\n",
        "df_ratio_2016_now = df.loc[dt.date(2016, 1, 1):] / df.loc[dt.date(2016, 1, 4)] * 100\n",
        "df_ratio_2016_now.head(3)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_ratio_2016_now['S&P500'])\n",
        "plt.plot(df_ratio_2016_now['KOSPI200'])\n",
        "plt.legend(loc=0)\n",
        "plt.grid(True, color='0.7', linestyle=':', linewidth=1)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(df_ratio_2016_now['S&P500'], df_ratio_2016_now['KOSPI200'], marker='.')\n",
        "plt.grid(True, color='0.7', linestyle=':', linewidth=1)\n",
        "plt.xlabel('S&P500')\n",
        "plt.ylabel('KOSPI200')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "x = df_ratio_2016_now['S&P500']\n",
        "y = df_ratio_2016_now['KOSPI200']\n",
        "\n",
        "# 1개 컬럼 np.array로 변환\n",
        "independent_var = np.array(x).reshape(-1, 1)\n",
        "dependent_var = np.array(y).reshape(-1, 1)\n",
        "\n",
        "# Linear Regression\n",
        "regr = LinearRegression()\n",
        "regr.fit(independent_var, dependent_var)\n",
        "\n",
        "result = {'Slope':regr.coef_[0,0], 'Intercept':regr.intercept_[0], 'R^2':regr.score(independent_var, dependent_var) }\n",
        "result\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(independent_var, dependent_var, marker='.', color='skyblue')\n",
        "plt.plot(independent_var, regr.predict(independent_var), color='r', linewidth=3)\n",
        "plt.grid(True, color='0.7', linestyle=':', linewidth=1)\n",
        "plt.xlabel('S&P500')\n",
        "plt.ylabel('KOSPI200')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}