{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwjang0902/ExampleNew/blob/master/RNN_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Wv_QqsvdzGqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "참조사이트\n",
        "    1. github.com/hjkim666/tensorflow\n",
        "    2. www.easy-tensorflow.com/tf-tutorials/recurrent-neural-networks/"
      ]
    },
    {
      "metadata": {
        "id": "8dcWw-urokJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 국방과학연구소/딥러닝의 이해\n",
        "# www.github/hjkim666/tensorflow 를 참조할 것\n",
        "#http://www.easy-tensorflow.com/tf-tutorials/recurrent-neural-networks/vanilla-rnn-for-classification\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Example-1\n",
        "'''\n",
        "hidden_size = 2\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units = hidden_size)\n",
        "\n",
        "x_data = np.array([[[1.,0.,0.,0.]]], dtype = np.float32)\n",
        "\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype = tf.float32)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "print(outputs.eval())\n",
        "'''\n",
        "'''\n",
        "# Example-2\n",
        "h = [1,0,0,0]\n",
        "e = [0,1,0,0]\n",
        "l = [0,0,1,0]\n",
        "o = [0,0,0,1]\n",
        "\n",
        "x_data = np.array([\n",
        "                  [h,e,l,l,o],\n",
        "                  [e,o,l,l,l],\n",
        "                  [l,l,e,e,l]\n",
        "                  ], dtype = np.float32)\n",
        "print(x_data.shape)\n",
        "#print(\"x_data = \", x_data)\n",
        "\n",
        "hidden_size = 2\n",
        "\n",
        "cell = tf.contrib.rnn.BasicRNNCell(num_units = hidden_size)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype = tf.float32)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "print(outputs.eval())\n",
        "print(outputs.eval().shape)\n",
        "'''\n",
        "# Example-3\n",
        "\n",
        "h = [1,0,0,0,0]\n",
        "i = [0,1,0,0,0]\n",
        "e = [0,0,1,0,0]\n",
        "l = [0,0,0,1,0]\n",
        "o = [0,0,0,0,1]\n",
        "\n",
        "idx2char =['h','i','e', 'l', 'o']\n",
        "\n",
        "x_one_hot = [[\n",
        "              [1,0,0,0,0],\n",
        "              [0,1,0,0,0],\n",
        "              [1,0,0,0,0],\n",
        "              [0,0,1,0,0],\n",
        "              [0,0,0,1,0],\n",
        "              [0,0,0,1,0]   \n",
        "            ]]\n",
        "\n",
        "x_data = [[0,1,0,2,3,3]]\n",
        "y_data = [[1,0,2,3,3,4]]\n",
        "\n",
        "hidden_size =5\n",
        "input_dim = 5\n",
        "batch_size = 1\n",
        "sequence_length = 6\n",
        "num_classes = 5\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "\n",
        "#forward section\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n",
        "\n",
        "# fully connected layer를 사용하는 경우임, 이 에제에서는 사용하지 않는 경우와 유사함...\n",
        "x_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "results = tf.contrib.layers.fully_connected(inputs = x_for_fc, num_outputs = num_classes, activation_fn = None)\n",
        "results = tf.reshape(results,[batch_size, sequence_length, num_classes])\n",
        "\n",
        "prediction = tf.argmax(outputs, axis = 2)\n",
        "\n",
        "# Training section\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
        "loss = tf.reduce_mean(sequence_loss)\n",
        "\n",
        "train = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
        "\n",
        "with tf.Session() as sess :\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(1000) :\n",
        "        L, _ = sess.run([loss, train], feed_dict = {X:x_one_hot, Y:y_data})\n",
        "        result = sess.run(prediction, feed_dict = {X:x_one_hot})\n",
        "        #print(i, \"loss\", L, \"prediction :\", result, \"true Y:\", y_data)\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)] # 숫자를 글자로 바꾸고, 디멘젼을 스퀴즈 한다.\n",
        "        #print(result_str)\n",
        "        print(i, \"\\t Predicted str \", \",\".join(result_str) ) #스트링으로 바꾼다\n",
        "        \n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lf9aXfectkMN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 국방과학연구소/딥러닝의 이해 : char-rnn(Example-4)   \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "\n",
        "sample = \"if you want you\"\n",
        "idx2char = list(set(sample))\n",
        "char2idx = {c:i for i,c in enumerate(idx2char)}\n",
        "print(sample, \" ->\", idx2char)\n",
        "print(char2idx)\n",
        "\n",
        "dic_size = len(char2idx)\n",
        "hidden_size = len(idx2char)\n",
        "num_classes = len(char2idx)\n",
        "batch_size = 1\n",
        "sequence_length = len(sample)-1\n",
        "\n",
        "sample_idx = [char2idx[c] for c in sample]\n",
        "print(sample_idx, \"length = \", len(sample_idx))\n",
        "\n",
        "x_data = [sample_idx[:-1]]\n",
        "y_data = [sample_idx[1:]]\n",
        "print(x_data,\"\\n\", y_data)\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "\n",
        "x_one_hot = tf.one_hot(X, num_classes)\n",
        "\n",
        "#forward section\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
        "initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell,x_one_hot, initial_state = initial_state, dtype = tf.float32)\n",
        "\n",
        "\n",
        "# fully connected layer를 사용하는 경우임, 이 에제에서는 사용하지 않는 경우와 유사함...\n",
        "x_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "results = tf.contrib.layers.fully_connected(inputs = x_for_fc, num_outputs = num_classes, activation_fn = None)\n",
        "results = tf.reshape(results,[batch_size, sequence_length, num_classes])\n",
        "\n",
        "prediction = tf.argmax(outputs, axis = 2)\n",
        "\n",
        "# Training section\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
        "loss = tf.reduce_mean(sequence_loss)\n",
        "\n",
        "train = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
        "\n",
        "with tf.Session() as sess :\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(20) :\n",
        "        L, _ = sess.run([loss, train], feed_dict = {X:x_data, Y:y_data})\n",
        "        result = sess.run(prediction, feed_dict = {X:x_data})\n",
        "        #print(i, \"loss\", L, \"prediction :\", result, \"true Y:\", y_data)\n",
        "        result_str = [idx2char[c] for c in np.squeeze(result)] # 숫자를 글자로 바꾸고, 디멘젼을 스퀴즈 한다.\n",
        "        #print(result_str)\n",
        "        print(i, \"\\t Predicted str \", \"\".join(result_str) ) #스트링으로 바꾼다\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f34iFugjBIml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "T0YFDIzX4Udv",
        "colab_type": "code",
        "outputId": "c76c647a-1353-4a4a-8477-e0958a51a7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "cell_type": "code",
      "source": [
        "# 국방과학연구소/딥러닝의 이해 : char-rnn 예제 2 (Example-5)   \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "sentence = (\"뉴스 기반의 지식네트워크를 지향하는\"\n",
        "            \"조선닷컴은 1995년 국내 최초로 온라인 뉴스서비스를 실시하여,\"\n",
        "            \"가장 빠른 국내 최고의 온라인 뉴스를 제공합니다.\")\n",
        "\n",
        "char_set = list(set(sentence)) \n",
        "char_dic = {c:i for i,c in enumerate(char_set)} # char dictionary\n",
        "print(sentence, \"\\n ->\", char_set)\n",
        "print(char_set)\n",
        "\n",
        "data_dim = len(char_set); print(\"data_dim = \", data_dim)\n",
        "num_classes = len(char_set); print(\"num_classes = \", num_classes)\n",
        "hidden_size = len(char_set); print(\"hidden_size = \", hidden_size)\n",
        "seq_length = 10\n",
        "\n",
        "#batch_size = 1\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, len(sentence)-seq_length) :\n",
        "    x_str = sentence[i:i+seq_length]\n",
        "    y_str = sentence[i+1:i+seq_length+1]\n",
        "#    print(i, x_str, '->', y_str)\n",
        "    \n",
        "    x = [char_dic[c] for c in x_str] \n",
        "    y = [char_dic[c] for c in y_str] \n",
        "    dataX.append(x)\n",
        "    dataY.append(y)\n",
        "\n",
        "#    print(dataX[i], '->', dataY[i])\n",
        "    \n",
        "batch_size = len(dataX); print(batch_size)    \n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, seq_length])\n",
        "Y = tf.placeholder(tf.int32, [None, seq_length])\n",
        "Batch_Size = tf.placeholder(tf.int32,[1])\n",
        "\n",
        "x_one_hot = tf.one_hot(X, num_classes)\n",
        "\n",
        "#forward section\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
        "initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell,x_one_hot, initial_state = initial_state, dtype = tf.float32)\n",
        "\n",
        "x_for_softmax = tf.reshape(outputs, [-1,hidden_size])\n",
        "softmax_w = tf.get_variable(\"softmax_w\",[hidden_size,num_classes])\n",
        "softmax_b = tf.get_variable(\"softmax_b\",[num_classes])\n",
        "outpputs = tf.matmul(x_for_softmax, softmax_w) + softmax_b\n",
        "\n",
        "outputs = tf.reshape(outputs,[batch_size, seq_length, num_classes])\n",
        "\n",
        "weights = tf.ones([batch_size, seq_length])\n",
        "\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
        "mean_loss = tf.reduce_mean(sequence_loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(0.01).minimize(mean_loss)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(1000) :\n",
        "    _, L, results = sess.run(\n",
        "            [train_op, mean_loss, outputs], feed_dict = {X:dataX, Y:dataY})\n",
        "    for j, result in enumerate(results) :\n",
        "        index = np.argmax(result, axis = 1)\n",
        "#        print(i,j, ''.join(char_set[t] for t in index), L)\n",
        "\n",
        "\n",
        "results = sess.run(outputs, feed_dict={X: dataX})\n",
        "for j, result in enumerate(results) :\n",
        "    index = np.argmax(result, axis = 1)\n",
        "    if j is 0 :\n",
        "        print( ''.join(char_set[t] for t in index), end='')\n",
        "    else :\n",
        "        print(char_set[index[-1]], end = '')\n",
        "        \n",
        "        \n",
        "\n",
        "# 입력으로 한개의 시퀀스만을 사용할 수 있도록 고쳐보자 ??????\n",
        "# 어떻게 고쳐야 하는지 모르겠당"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "뉴스 기반의 지식네트워크를 지향하는조선닷컴은 1995년 국내 최초로 온라인 뉴스서비스를 실시하여,가장 빠른 국내 최고의 온라인 뉴스를 제공합니다. \n",
            " -> ['로', '고', '합', ' ', '온', '크', '여', ',', '하', '장', '빠', '.', '인', '9', '국', '실', '다', '최', '1', '조', '제', '뉴', '비', '라', '의', '지', '선', '트', '내', '서', '시', '른', '향', '네', '5', '스', '닷', '년', '는', '컴', '식', '초', '를', '은', '공', '반', '니', '기', '워', '가']\n",
            "['로', '고', '합', ' ', '온', '크', '여', ',', '하', '장', '빠', '.', '인', '9', '국', '실', '다', '최', '1', '조', '제', '뉴', '비', '라', '의', '지', '선', '트', '내', '서', '시', '른', '향', '네', '5', '스', '닷', '년', '는', '컴', '식', '초', '를', '은', '공', '반', '니', '기', '워', '가']\n",
            "data_dim =  50\n",
            "num_classes =  50\n",
            "hidden_size =  50\n",
            "71\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "스서기반의 지식네트워크를 지향하는조선닷컴은 1995년 국내 최초로 온라인 뉴스서비스를 실시하여,가장 빠른 국내 최고의 온라인 뉴스를 제공합니다니"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CD3mXM2BZZnn",
        "colab_type": "code",
        "outputId": "86d7d67e-ed0c-4b19-e1a4-b0a27871c7db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "cell_type": "code",
      "source": [
        "# 국방과학연구소/딥러닝의 이해 : char-rnn 예제 2 (Example-5)  New ; Batch-size와 관계없이 만들어진 것임 .....\n",
        "# 2019.4.24\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "sentence = (\"뉴스 기반의 지식네트워크를 지향하는\"\n",
        "            \"조선닷컴은 1995년 국내 최초로 온라인 뉴스서비스를 실시하여,\"\n",
        "            \"가장 빠른 국내 최고의 온라인 뉴스를 제공합니다.\")\n",
        "\n",
        "char_set = list(set(sentence)) \n",
        "char_dic = {c:i for i,c in enumerate(char_set)} # char dictionary\n",
        "print(sentence, \"\\n ->\", char_set)\n",
        "print(char_set)\n",
        "\n",
        "data_dim = len(char_set); print(\"data_dim = \", data_dim)\n",
        "num_classes = len(char_set); print(\"num_classes = \", num_classes)\n",
        "hidden_size = len(char_set); print(\"hidden_size = \", hidden_size)\n",
        "seq_length = 10\n",
        "\n",
        "#batch_size = 1\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, len(sentence)-seq_length) :\n",
        "    x_str = sentence[i:i+seq_length]\n",
        "    y_str = sentence[i+1:i+seq_length+1]\n",
        "#    print(i, x_str, '->', y_str)\n",
        "    \n",
        "    x = [char_dic[c] for c in x_str] \n",
        "    y = [char_dic[c] for c in y_str] \n",
        "    dataX.append(x)\n",
        "    dataY.append(y)\n",
        "\n",
        "#    print(dataX[i], '->', dataY[i])\n",
        "    \n",
        "batch_size = len(dataX); print(batch_size)    \n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, seq_length])\n",
        "Y = tf.placeholder(tf.int32, [None, seq_length])\n",
        "Batch_Size = tf.placeholder(tf.int32,[1])\n",
        "\n",
        "x_one_hot = tf.one_hot(X, num_classes)\n",
        "\n",
        "#forward section\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, state_is_tuple = True)\n",
        "#initial_state = cell.zero_state(batch_size , tf.float32)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell,x_one_hot, dtype = tf.float32)\n",
        "\n",
        "'''\n",
        "x_for_softmax = tf.reshape(outputs, [-1,hidden_size])\n",
        "softmax_w = tf.get_variable(\"softmax_w\",[hidden_size,num_classes])\n",
        "softmax_b = tf.get_variable(\"softmax_b\",[num_classes])\n",
        "outpputs = tf.matmul(x_for_softmax, softmax_w) + softmax_b\n",
        "\n",
        "\n",
        "outputs = tf.reshape(outputs,[batch_size, seq_length, num_classes])\n",
        "\n",
        "# weights = tf.ones([batch_size , seq_length])\n",
        "'''\n",
        "\n",
        "#sequence_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = Y, weights = weights)\n",
        "sequence_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=outputs) # 새롭게 사용한 것임; batch_size를 사용하지 않기 위해서 ...\n",
        "mean_loss = tf.reduce_mean(sequence_loss)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(0.001).minimize(mean_loss)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(10000) :\n",
        "    _, L, results = sess.run(\n",
        "            [train_op, mean_loss, outputs], feed_dict = {X:dataX, Y:dataY})\n",
        "    for j, result in enumerate(results) :\n",
        "        index = np.argmax(result, axis = 1)\n",
        "#        print(i,j, ''.join(char_set[t] for t in index), L)\n",
        "\n",
        "\n",
        "results = sess.run(outputs, feed_dict={X:dataX[0:30]})\n",
        "for j, result in enumerate(results) :\n",
        "    index = np.argmax(result, axis = 1)\n",
        "    if j is 0 :\n",
        "        print( ''.join(char_set[t] for t in index), end='')\n",
        "    else :\n",
        "        print(char_set[index[-1]], end = '')\n",
        "        \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "뉴스 기반의 지식네트워크를 지향하는조선닷컴은 1995년 국내 최초로 온라인 뉴스서비스를 실시하여,가장 빠른 국내 최고의 온라인 뉴스를 제공합니다. \n",
            " -> ['로', '고', '합', ' ', '온', '크', '여', ',', '하', '장', '빠', '.', '인', '9', '국', '실', '다', '최', '1', '조', '제', '뉴', '비', '라', '의', '지', '선', '트', '내', '서', '시', '른', '향', '네', '5', '스', '닷', '년', '는', '컴', '식', '초', '를', '은', '공', '반', '니', '기', '워', '가']\n",
            "['로', '고', '합', ' ', '온', '크', '여', ',', '하', '장', '빠', '.', '인', '9', '국', '실', '다', '최', '1', '조', '제', '뉴', '비', '라', '의', '지', '선', '트', '내', '서', '시', '른', '향', '네', '5', '스', '닷', '년', '는', '컴', '식', '초', '를', '은', '공', '반', '니', '기', '워', '가']\n",
            "data_dim =  50\n",
            "num_classes =  50\n",
            "hidden_size =  50\n",
            "71\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "스서기반의 지식네트워크를 지향하는조선닷컴은 1995년 국내 최초로 온라"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J1lxzgdY5vS7",
        "colab_type": "code",
        "outputId": "2851a7b0-e805-47e6-a0a0-66abeb503220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "cell_type": "code",
      "source": [
        "# 머신러닝 학습의 Hello World 와 같은 MNIST(손글씨 숫자 인식) 문제를 신경망으로 풀어봅니다.\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "\n",
        "#########\n",
        "# 옵션 설정\n",
        "######\n",
        "learning_rate = 0.001\n",
        "total_epoch = 30\n",
        "batch_size = 128\n",
        "\n",
        "# RNN 은 순서가 있는 자료를 다루므로,\n",
        "# 한 번에 입력받는 갯수와, 총 몇 단계로 이루어져있는 데이터를 받을지를 설정해야합니다.\n",
        "# 이를 위해 가로 픽셀수를 n_input 으로, 세로 픽셀수를 입력 단계인 n_step 으로 설정하였습니다.\n",
        "n_input = 28\n",
        "n_step = 28\n",
        "n_sequence = n_step\n",
        "n_hidden = 128\n",
        "n_class = 10\n",
        "\n",
        "#########\n",
        "# 신경망 모델 구성\n",
        "######\n",
        "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
        "\n",
        "Y = tf.placeholder(tf.float32, [None, n_class])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
        "b = tf.Variable(tf.random_normal([n_class]))\n",
        "\n",
        "# RNN 에 학습에 사용할 셀을 생성합니다\n",
        "# 다음 함수들을 사용하면 다른 구조의 셀로 간단하게 변경할 수 있습니다\n",
        "# BasicRNNCell,BasicLSTMCell,GRUCell\n",
        "cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "\n",
        "# RNN 신경망을 생성합니다\n",
        "# 원래는 다음과 같은 과정을 거쳐야 하지만\n",
        "# states = tf.zeros(batch_size)\n",
        "# for i in range(n_step):\n",
        "#     outputs, states = cell(X[[:, i]], states)\n",
        "# ...\n",
        "# 다음처럼 tf.nn.dynamic_rnn 함수를 사용하면\n",
        "# CNN 의 tf.nn.conv2d 함수처럼 간단하게 RNN 신경망을 만들어줍니다.\n",
        "# 겁나 매직!!\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "# RNN의 출력이다.\n",
        "\n",
        "# 결과를 Y의 다음 형식과 바꿔야 하기 때문에\n",
        "# Y : [batch_size, n_class]\n",
        "# outputs 의 형태를 이에 맞춰 변경해야합니다.\n",
        "# outputs : [batch_size, n_step, n_hidden]\n",
        "#        -> [n_step, batch_size, n_hidden]\n",
        "#        -> [batch_size, n_hidden]\n",
        "#outputs = tf.transpose(outputs, [1, 0, 2])\n",
        "outputs = outputs[:,-1,:] # 시퀀스에서 하나를 선택한 것이다.\n",
        "#outputs = outputs[-2] # n_step는 시퀀스, 즉 cell의 수이고 안에 숫자는 몇 번째 셀의 출력을 취할 것인가를 정하는 것으로 보면 된다 !!!!!!!!\n",
        "model = tf.matmul(outputs, W) + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "#########\n",
        "# 신경망 모델 학습\n",
        "######\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        # X 데이터를 RNN 입력 데이터에 맞게 [batch_size, n_step, n_input] 형태로 변환합니다.\n",
        "        batch_xs = batch_xs.reshape((batch_size, n_step, n_input))\n",
        "\n",
        "        _, cost_val = sess.run([optimizer, cost],\n",
        "                               feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        total_cost += cost_val\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
        "\n",
        "print('최적화 완료!')\n",
        "\n",
        "#########\n",
        "# 결과 확인\n",
        "######\n",
        "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "test_batch_size = len(mnist.test.images)\n",
        "test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input)\n",
        "test_ys = mnist.test.labels\n",
        "\n",
        "print('정확도:', sess.run(accuracy,\n",
        "                       feed_dict={X: test_xs, Y: test_ys}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Avg. cost = 0.540\n",
            "Epoch: 0002 Avg. cost = 0.237\n",
            "Epoch: 0003 Avg. cost = 0.182\n",
            "Epoch: 0004 Avg. cost = 0.151\n",
            "Epoch: 0005 Avg. cost = 0.130\n",
            "Epoch: 0006 Avg. cost = 0.119\n",
            "Epoch: 0007 Avg. cost = 0.120\n",
            "Epoch: 0008 Avg. cost = 0.107\n",
            "Epoch: 0009 Avg. cost = 0.097\n",
            "Epoch: 0010 Avg. cost = 0.097\n",
            "Epoch: 0011 Avg. cost = 0.089\n",
            "Epoch: 0012 Avg. cost = 0.089\n",
            "Epoch: 0013 Avg. cost = 0.085\n",
            "Epoch: 0014 Avg. cost = 0.091\n",
            "Epoch: 0015 Avg. cost = 0.082\n",
            "Epoch: 0016 Avg. cost = 0.082\n",
            "Epoch: 0017 Avg. cost = 0.077\n",
            "Epoch: 0018 Avg. cost = 0.074\n",
            "Epoch: 0019 Avg. cost = 0.075\n",
            "Epoch: 0020 Avg. cost = 0.076\n",
            "Epoch: 0021 Avg. cost = 0.067\n",
            "Epoch: 0022 Avg. cost = 0.069\n",
            "Epoch: 0023 Avg. cost = 0.068\n",
            "Epoch: 0024 Avg. cost = 0.062\n",
            "Epoch: 0025 Avg. cost = 0.060\n",
            "Epoch: 0026 Avg. cost = 0.065\n",
            "Epoch: 0027 Avg. cost = 0.063\n",
            "Epoch: 0028 Avg. cost = 0.065\n",
            "Epoch: 0029 Avg. cost = 0.065\n",
            "Epoch: 0030 Avg. cost = 0.062\n",
            "최적화 완료!\n",
            "정확도: 0.9724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhcYrN3y0VoB",
        "colab_type": "code",
        "outputId": "b13e6015-7cc6-449a-92b4-8a3abc3f57b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "cell_type": "code",
      "source": [
        "# MNIST 데이터 읽기 및 Ploting\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "\n",
        "# MNIST DATA READING\n",
        "mnist_images = input_data.read_data_sets(\"./mnist/data/\", one_hot=False)\n",
        "\n",
        "train_images,train_labels = mnist_images.train.next_batch(50000)\n",
        "valid_images,valid_labels = mnist_images.train.next_batch(10000)\n",
        "test_images, test_labels = mnist_images.test.next_batch(10000)\n",
        "\n",
        "n = 5001\n",
        "fig  =test_images[n].reshape([28,28])\n",
        "plt.imshow(fig)\n",
        "plt.title(test_labels[n])\n",
        "\n",
        "print(len(train_images),len(valid_images),len(test_images) )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
            "50000 10000 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wGS3V0HTdjFQAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "U0nZNkaEPsEj",
        "colab_type": "code",
        "outputId": "5635dfdc-5db4-44a6-856f-0df41898ca78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "# MNIST 데이터 읽기 및 Ploting from Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pylab as plt\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 #normalization(0~1)\n",
        "\n",
        "plt.imshow(x_train[0].reshape(28,28))\n",
        "plt.title(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}