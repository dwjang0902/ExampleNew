{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwjang0902/ExampleNew/blob/master/Data_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pdrViFnfbut1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------\n",
        "#주가 데이터 읽어오기\n",
        "#---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph() # 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 한국거래소에서 종목코드 가져오기\n",
        "code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13', header=0)[0]\n",
        "\n",
        "# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌 \n",
        "code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)\n",
        "\n",
        "# 우리가 필요한 것은 회사명과 종목코드이기 때문에 필요없는 column들은 제외해준다. \n",
        "code_df = code_df[['회사명', '종목코드']]\n",
        "\n",
        "# 한글로된 컬럼명을 영어로 바꿔준다. \n",
        "code_df = code_df.rename(columns={'회사명': 'name', '종목코드': 'code'}) \n",
        "code_df.head()\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# 종목 이름을 입력하면 종목에 해당하는 코드를 불러와 \n",
        "# 네이버 금융(http://finance.naver.com)에 넣어줌 \n",
        "def get_url(item_name, code_df): \n",
        "    code = code_df.query(\"name=='{}'\".format(item_name))['code'].to_string(index=False) \n",
        "    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code) \n",
        "    \n",
        "    print(\"요청 URL = {}\".format(url)) \n",
        "    return url, code\n",
        "\n",
        "# 신라젠의 일자데이터 url 가져오기 \n",
        "\n",
        "item_name='코메론' \n",
        "url, code = get_url(item_name, code_df)\n",
        "#print(type(code))\n",
        "# 왜 이렇게 해야 동작하는지 잘 모르겠다.\n",
        "code = np.int(code)\n",
        "code = str(code)\n",
        "code = code.zfill(6)\n",
        "\n",
        "code_num = code\n",
        "#print(url)\n",
        "\n",
        "# 일자 데이터를 담을 df라는 DataFrame 정의 \n",
        "df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# 1페이지에서 20페이지의 데이터만 가져오기 \n",
        "for page in range(1, 20): \n",
        "#    pg_url = '{url}&page={page}'.format(url=url, page=page) # 동작을 않는다, 이유를 모르겠다 ???\n",
        "    pg_url = 'http://finance.naver.com/item/sise_day.nhn?code=' + code_num + '&page='+ str(page)\n",
        "#    print(pg_url)\n",
        "#    df = pd.read_html(pg_url)\n",
        "    df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)\n",
        " \n",
        "# NaN 데이타 제거\n",
        "df = df.dropna(axis = 0)\n",
        "\n",
        "# 한글로 된 컬럼명을 영어로 바꿔줌 \n",
        "df = df.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'}) \n",
        "\n",
        "# 데이터의 타입을 int형으로 바꿔줌 \n",
        "#df[['close', 'diff', 'open', 'high', 'low', 'volume']]  = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(float) \n",
        "\n",
        "# 일자(date)를 기준으로 오름차순 정렬 \n",
        "df = df.sort_values(by=['date'], ascending=True) \n",
        "\n",
        "# 상위 5개 데이터 확인\n",
        "#print( df.tail(10))\n",
        "\n",
        "plt.plot(df[\"close\"].to_numpy())\n",
        "plt.show()\n",
        "\n",
        "#print(df.dtypes) #object\n",
        "\n",
        "#------------------------------------------------------------\n",
        "# 예측하기 by RNN\n",
        "#------------------------------------------------------------\n",
        "input_data = df.as_matrix()[:, 1:7].astype(int)\n",
        "input_data = df.to_numpy()[:, 1:7].astype(int)\n",
        "print(type(input_data), input_data.shape)\n",
        "print(input_data[0])\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "#-----------------------------\n",
        "from sklearn.preprocessing import normalize\n",
        "input_data = normalize(input_data, axis = 0, norm = 'l1') #column normalize\n",
        "input_data = np.asarray(input_data)\n",
        "print(type(input_data), input_data.shape)\n",
        "print(input_data[0])\n",
        "\n",
        "\n",
        "# RNN 입력데이터 준비\n",
        "#-------------------------------\n",
        "def gen_xy_data(input_data, seq_len, pred_step = 1) :\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        length = input_data.shape[0] - seq_len - pred_step+1\n",
        "        print(\" length = \", length)\n",
        "        for i in range(length) :\n",
        "            x = input_data[i:i+seq_len]\n",
        "            y = input_data[i+pred_step: i+seq_len+pred_step]\n",
        "            x_data.append(x)\n",
        "            y_data.append(y)\n",
        "        return x_data, y_data\n",
        "\n",
        "seq_len   = 2\n",
        "pred_step = 1\n",
        "#x_data = [batch_size, seq_len, input_dim]; y_data = [batch_size, seq_len, output_dim]\n",
        "x_data, y_data = gen_xy_data(input_data, seq_len, pred_step)\n",
        "x_data = np.asarray(x_data)\n",
        "y_data = np.asarray(y_data)\n",
        "print(type(x_data), x_data.shape)\n",
        "print(type(y_data), y_data.shape)\n",
        "print(x_data[0:1])\n",
        "print(y_data[0:1])\n",
        "\n",
        "# 데이터 분리(트레이닝, 테스트)\n",
        "import sklearn\n",
        "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x_data, y_data, test_size=0.33, random_state= None)\n",
        "print(len(x_train), len(x_test))\n",
        "\n",
        "#Parameter\n",
        "#----------------------------\n",
        "input_dim = x_train.shape[2] #6\n",
        "output_dim = 1 # 종가만 예측하는 것으로 한다\n",
        "seq_len = seq_len\n",
        "hidden_size = 10\n",
        "\n",
        "#Model\n",
        "X = placeholder(tf.float32, [None, seq_len, input_dim])\n",
        "Y = placeholder(tf.float32, [None, seq_len, output_dim])\n",
        "\n",
        "# 참조 : https://github.com/tensorflow/tensorflow/issues/16186\n",
        "def lstm_cell():\n",
        "   lstm = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=1.0)\n",
        "   return lstm\n",
        "cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)])\n",
        "outputs, _states = tf.nn.dynamic_rnn(cells, x, dtype = tf.float32) # 출력\n",
        "#outputs = [batch_size, seq_len, hidden_size  ]\n",
        "\n",
        "w = tf.variables(tf.float32, tf.random_normal([out_dim, hidden_size]))\n",
        "b = tf.variables(tf.float32, tf.random_normal([out_dim]))\n",
        "outputs = [tf.matmul(w,outputs) + b for i in outputs]\n",
        "#[batch_size, seq_len, out_dim]\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "#stacked_rnn_outputs = tf.reshape(outputs, [-1, hidden])\n",
        "#stacked_outputs = tf.layers.dense(stacked_rnn_outputs , output_dim)\n",
        "#outputs = tf.reshape(stacked_outputs, [-1, seq_len, output_dim])\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - y[:,:,0]))\n",
        "#여기서 y는 [:,:,0], 즉 종가만을 예측하는 것으로 한다.\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\"\"\"\n",
        "epoches = 10000\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "with tf.Session() as sess :\n",
        "    sess.run(init)\n",
        "    for ep in range(epoches) :\n",
        "        sess.run(training_op, feed_dict = { x : x_batches, y: y_batches})\n",
        "        \n",
        "        if ep % 100 == 0 :\n",
        "            mse = loss.eval(feed_dict = { x : x_batches, y: y_batches})\n",
        "#            print(ep, \"\\MSE \", mse)\n",
        "\n",
        "    y_pred = sess.run(outputs, feed_dict = {x:x_test})\n",
        "#    print(\" Prediction = \\n \", y_pred)\n",
        "        \n",
        "#------------------------------------------------------------------------------------------\n",
        "plt.title(\"Forecast vs Actual\", fontsize = 14)\n",
        "plt.plot( pd.Series(np.ravel(y_test)), \"bo\", markersize = 10, label =\"Actual\")\n",
        "\n",
        "plt.plot(pd.Series(np.ravel(y_pred)), \"r.\", markersize = 10, label = \"Forecast\")\n",
        "plt.legend(loc = \" upper right\")\n",
        "plt.xlabel(\"Time Periods\")\n",
        "plt.show\n",
        "        \n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}