{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_prediction_stock_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwjang0902/ExampleNew/blob/master/Data_prediction_stock_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdrViFnfbut1",
        "colab_type": "code",
        "outputId": "ef4f6769-6d77-422a-d11f-d7f6ed75251f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "#---------------------\n",
        "# 주가 데이터를 네이버 사이트에서 읽어와서, 종가를 예측하는 예제\n",
        "#참조 : https://coolingoff.tistory.com/category/%5B%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%5D/Tensorflow\n",
        "#       https://sshkim.tistory.com/153\n",
        "#       https://www.youtube.com/watch?v=odMGK7pwTqY\n",
        "#       Deep Learning ZerotoAll\n",
        "\n",
        "#2019. 5. 3\n",
        "#---------------------\n",
        "\n",
        "#---------------------------------------------------------\n",
        "#주가 데이터 읽어오기\n",
        "#---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.reset_default_graph() \n",
        "# 중요한 부분이다; 이것이 없으면 런타임을 항상 리셋 해 주어야 한다.\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 한국거래소에서 종목코드 가져오기\n",
        "url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'\n",
        "code_df = pd.read_html(url, header=0)[0]\n",
        "\n",
        "# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌 \n",
        "code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)\n",
        "\n",
        "# 우리가 필요한 것은 회사명과 종목코드이기 때문에 필요없는 column들은 제외해준다. \n",
        "code_df = code_df[['회사명', '종목코드']]\n",
        "\n",
        "# 한글로된 컬럼명을 영어로 바꿔준다. \n",
        "code_df = code_df.rename(columns={'회사명': 'name', '종목코드': 'code'}) \n",
        "print(code_df.head())\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 종목 이름을 입력하면 종목에 해당하는 코드를 불러와 네이버 금융(http://finance.naver.com)에 넣어줌 \n",
        "def get_url(item_name, code_df): \n",
        "    code = code_df.query(\"name=='{}'\".format(item_name))['code'].to_string(index=False) \n",
        "    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code) \n",
        "    print(\"요청 URL = {}\".format(url)) \n",
        "    print('종목명 = ',item_name, 'Code =', code)\n",
        "    print('URL = ', url)\n",
        "    return url, code\n",
        "\n",
        "\n",
        "\n",
        "# 종목명의 일자데이터 url 가져오기 \n",
        "item_name='신라젠' \n",
        "url, code = get_url(item_name, code_df)\n",
        "#print(type(code))\n",
        "# 왜 이렇게 해야 동작하는지 잘 모르겠다.\n",
        "code = np.int(code)\n",
        "code = str(code)\n",
        "code = code.zfill(6) # 자릿수 맞추기\n",
        "\n",
        "code_num = code\n",
        "\n",
        "# 일자 데이터를 담을 df라는 DataFrame 정의 \n",
        "df = pd.DataFrame()\n",
        "\n",
        "#-----------------------------------------------------\n",
        "# 마지막 페이지 찾기\n",
        "import urllib\n",
        "import time\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "stockItem = code\n",
        "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
        "html = urlopen(url)  \n",
        "source = BeautifulSoup(html.read(), \"html.parser\")\n",
        "maxPage=source.find_all(\"table\",align=\"center\")\n",
        "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
        "mpNum = int(mp[0].a.get('href')[-3:]) #마지막 페이지 찾는 것\n",
        "print(\"마지막 페이지 : \", mpNum )\n",
        "#--------------------------------------------------------------\n",
        "# 1페이지에서 20페이지의 데이터만 가져오기\n",
        "#마지막 페이지 찾는 방법 ???\n",
        "for page in range(1, 70): \n",
        "#    pg_url = '{url}&page={page}'.format(url=url, page=page) # 동작을 않는다, 이유를 모르겠다 ???\n",
        "    pg_url = 'http://finance.naver.com/item/sise_day.nhn?code=' + code_num + '&page='+ str(page)\n",
        "#    print(pg_url)\n",
        "#    df = pd.read_html(pg_url)\n",
        "    df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)\n",
        " \n",
        "# NaN 데이타 제거\n",
        "df = df.dropna(axis = 0)\n",
        "\n",
        "# 한글로 된 컬럼명을 영어로 바꿔줌 \n",
        "df = df.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'}) \n",
        "\n",
        "# 데이터의 타입을 int형으로 바꿔줌 \n",
        "#df[['close', 'diff', 'open', 'high', 'low', 'volume']]  = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(float) \n",
        "\n",
        "# 일자(date)를 기준으로 오름차순 정렬 \n",
        "df = df.sort_values(by=['date'], ascending=True) \n",
        "\n",
        "# 상위 5개 데이터 확인\n",
        "#print( df.tail(5))\n",
        "\n",
        "#------------------------------------------------------------\n",
        "# 예측하기 by RNN\n",
        "#------------------------------------------------------------\n",
        "#input_data = df.as_matrix()[:, 1:7].astype(int) \n",
        "input_data = df.to_numpy()[:, 1:7].astype(int)\n",
        "#print(type(input_data), input_data.shape)\n",
        "#print(input_data[0])\n",
        "\n",
        "\n",
        "# 데이터 스케일링\n",
        "#-----------------------------\n",
        "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale\n",
        "input_data = scale(input_data) \n",
        "input_data = np.asarray(input_data)\n",
        "\n",
        "# RNN 입력데이터 준비\n",
        "#-------------------------------\n",
        "def gen_xy_data(input_data, seq_len, pred_step = 1) :\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        length = input_data.shape[0] - seq_len - pred_step+1\n",
        "        print(\" length = \", length)\n",
        "        for i in range(length) :\n",
        "            x = input_data[i:i+seq_len]\n",
        "            y = input_data[i+pred_step: i+seq_len+pred_step]\n",
        "            x_data.append(x)\n",
        "            y_data.append(y)\n",
        "        return x_data, y_data\n",
        "\n",
        "seq_len   = 10 # time_step, RNN셀의 수\n",
        "pred_step = 1  # 몇번 앞을 예측할지를 결정\n",
        "#x_data = [batch_size, seq_len, input_dim]; y_data = [batch_size, seq_len, output_dim]\n",
        "x_data, y_data = gen_xy_data(input_data, seq_len, pred_step)\n",
        "x_data = np.asarray(x_data)\n",
        "y_data = np.asarray(y_data)\n",
        "\n",
        "#print(x_data[0:1])\n",
        "#print(y_data[0:1])\n",
        "\n",
        "# 데이터 분리(트레이닝, 테스트)\n",
        "#-------------------------------\n",
        "def train_test_split_self(x_data, y_data, test_size=0.30, self = 1, random_state = None):\n",
        "    if self == 0 : \n",
        "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.50, random_state = random_state)\n",
        "    else :\n",
        "        train_size = int(x_data.shape[0]*test_size)\n",
        "        x_train = x_data[0:train_size]; y_train = y_data[0:train_size]\n",
        "        x_test = x_data[train_size:]; y_test = y_data[train_size:]\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split_self(x_data, y_data, test_size=0.50, random_state= None)\n",
        "    \n",
        "y_train = y_train[:,:,0]\n",
        "y_test = y_test[:,:,0]\n",
        "y_train = np.expand_dims(y_train, axis = 2)\n",
        "y_test = np.expand_dims(y_test, axis = 2)\n",
        "\n",
        "print(type(x_train), x_train.shape)\n",
        "print(type(y_train), y_train.shape)\n",
        "#print(y_train[0])\n",
        "#print(y_test[0])\n",
        "\n",
        "plt.plot(y_test[:,1])\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# 학습 및 예측; RNN\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "#Parameter\n",
        "#----------------------------\n",
        "input_dim = x_train.shape[2] #6, 여기서는 [6]\n",
        "output_dim = 1 # 종가만 예측하는 것으로 한다\n",
        "seq_len = seq_len\n",
        "hidden_size = 30\n",
        "print(\"입력 디멘젼 : \",input_dim)\n",
        "print(\"출력 디멘젼 : \",output_dim)\n",
        "print(\"셀의 수 : \",seq_len)\n",
        "print(\"히든 사이즈 : \",hidden_size)\n",
        "\n",
        "#Model\n",
        "X = tf.placeholder(tf.float32, [None, seq_len, input_dim])\n",
        "Y = tf.placeholder(tf.float32, [None, seq_len, output_dim])\n",
        "\n",
        "# 참조 : https://github.com/tensorflow/tensorflow/issues/16186\n",
        "\n",
        "# 멀티 셀, forward\n",
        "#----------------------------\n",
        "def cell():\n",
        "   c = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n",
        "   #c = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
        "   #c = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
        "   return c\n",
        "\n",
        "num_mult = 1\n",
        "print(\"다중계층 수 = \",num_mult)\n",
        "cells = tf.contrib.rnn.MultiRNNCell([cell() for _ in range(num_mult)])\n",
        "outputs, _states = tf.nn.dynamic_rnn(cells, X, dtype = tf.float32)\n",
        "#outputs = [batch_size, seq_len, hidden_size]\n",
        "\n",
        "# 변수 정의 및 출력 reshape\n",
        "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, output_dim]), dtype=tf.float32)\n",
        "softmax_b = tf.Variable(tf.random_normal(shape=[output_dim]), dtype=tf.float32)\n",
        "output = tf.reshape(outputs, [-1, hidden_size]) #[batch_size*seq_len, hidden_size]\n",
        "\n",
        "\n",
        "# 최종 출력값을 설정합니다; 일종의 dense_layer 추가 ...\n",
        "#logits = tf.contrib.layers.fully_connected(output,output_dim)\n",
        "logits = tf.matmul(output, softmax_w) + softmax_b # logits : [batch_size * seq_length, output_dim]\n",
        "\n",
        "logits = tf.reshape(logits ,[-1, seq_len, output_dim]) #reshape\n",
        "#logits = tf.nn.relu(logits)\n",
        "#logits = tf.math.tanh(logits)\n",
        "predictions = logits\n",
        "\n",
        "# Loss, 트레이너 정의 등\n",
        "#----------------------------\n",
        "# 디멘젼에 주의 할 것\n",
        "loss = tf.losses.mean_squared_error(predictions = logits, labels = Y,  weights=1.0)\n",
        "loss_total = tf.reduce_mean(loss)  \n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss_total)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "epoches = 1000\n",
        "batch_size = 32\n",
        "num_batches = int(x_train.shape[0]/batch_size)\n",
        "print(\"Epoches = \", epoches)\n",
        "print(\"batch_size\",batch_size)\n",
        "print(\"num_batches\", num_batches)\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "with tf.Session() as sess :\n",
        "    sess.run(init)\n",
        "    for ep in range(epoches) :\n",
        "        sess.run(training_op, feed_dict = { X : x_train, Y: y_train}) #트레이닝\n",
        "        \"\"\"\n",
        "        for bh in range(num_batches-1) :\n",
        "            bh0 = int(bh*batch_size); bh1 = int((bh+1)*batch_size)\n",
        "            x_batches, y_batches = x_train[bh0:bh1], y_train[bh0:bh1] # 배치 만들기\n",
        "            sess.run(training_op, feed_dict = { X : x_batches, Y: y_batches}) # 트레이닝\n",
        "        \"\"\"\n",
        "        \n",
        "        if ep % 100 == 0 :\n",
        "            mse = loss.eval(feed_dict = { X : x_test, Y: y_test})\n",
        "\n",
        "    y_pred = sess.run(predictions, feed_dict = {X: x_test}) # 예측\n",
        "    #    print(\" Prediction = \\n \", np.squeeze(y_pred))\n",
        "\n",
        "#결과 그리기    \n",
        "plt.figure(2)\n",
        "plt.title(\"Prediction vs Actual\", fontsize = 14)\n",
        "plt.plot(np.squeeze(y_test)[:,1], label = \"Actual\") \n",
        "plt.plot(np.squeeze(y_pred)[:,1], label =\"Prediction\") \n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.xlabel(\"Time Periods\")\n",
        "plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    name    code\n",
            "0  GS글로벌  001250\n",
            "1  HSD엔진  082740\n",
            "2  KG케미칼  001390\n",
            "3  LG이노텍  011070\n",
            "4    OCI  010060\n",
            "요청 URL = http://finance.naver.com/item/sise_day.nhn?code= 215600\n",
            "종목명 =  신라젠 Code =  215600\n",
            "URL =  http://finance.naver.com/item/sise_day.nhn?code= 215600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d9c9da3410c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mmaxPage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"center\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxPage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"td\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pgRR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mmpNum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#마지막 페이지 찾는 것\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"마지막 페이지 : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmpNum\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#--------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '=59'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XrXaq0tIG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6ea698b7-89ba-4e61-fc5f-3dea75d9055d"
      },
      "source": [
        "import urllib\n",
        "import time\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "stockItem = '005930'\n",
        "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
        "html = urlopen(url)  \n",
        "source = BeautifulSoup(html.read(), \"html.parser\")\n",
        "maxPage=source.find_all(\"table\",align=\"center\")\n",
        "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
        "mpNum = int(mp[0].a.get('href')[-3:])\n",
        "print(mpNum)\n",
        "\n",
        "\"\"\"                                            \n",
        "\n",
        "for page in range(1, mpNum+1):\n",
        "\n",
        "  print (str(page) )\n",
        "\n",
        "  url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
        "\n",
        "  html = urlopen(url)\n",
        "\n",
        "  source = BeautifulSoup(html.read(), \"html.parser\")\n",
        "\n",
        "  srlists=source.find_all(\"tr\") \n",
        "\n",
        "  isCheckNone = None\n",
        "\n",
        "   \n",
        "\n",
        "  if((page % 1) == 0):\n",
        "\n",
        "    time.sleep(1.50)\n",
        "\n",
        " \n",
        "\n",
        "  for i in range(1,len(srlists)-1): \n",
        "\n",
        "   if(srlists[i].span != isCheckNone):\n",
        "\n",
        "     \n",
        "\n",
        "    srlists[i].td.text\n",
        "\n",
        "    print(srlists[i].find_all(\"td\",align=\"center\")[0].text, srlists[i].find_all(\"td\",class_=\"num\")[0].text )\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                            \\n\\nfor page in range(1, mpNum+1):\\n\\n  print (str(page) )\\n\\n  url = \\'http://finance.naver.com/item/sise_day.nhn?code=\\' + stockItem +\\'&page=\\'+ str(page)\\n\\n  html = urlopen(url)\\n\\n  source = BeautifulSoup(html.read(), \"html.parser\")\\n\\n  srlists=source.find_all(\"tr\") \\n\\n  isCheckNone = None\\n\\n   \\n\\n  if((page % 1) == 0):\\n\\n    time.sleep(1.50)\\n\\n \\n\\n  for i in range(1,len(srlists)-1): \\n\\n   if(srlists[i].span != isCheckNone):\\n\\n     \\n\\n    srlists[i].td.text\\n\\n    print(srlists[i].find_all(\"td\",align=\"center\")[0].text, srlists[i].find_all(\"td\",class_=\"num\")[0].text )\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZCxZD5F3iNw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}